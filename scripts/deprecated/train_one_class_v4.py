#!/usr/bin/env python3
"""
Train One-Class v4 Model with Enhanced Features
YAMNet ì„ë² ë”© + ìŒí–¥ íŠ¹ì„± (MFCC, Spectral features ë“±)
"""

import sys
import numpy as np
import librosa
from pathlib import Path
import pickle
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

sys.path.append(str(Path(__file__).parent / "src"))

from models.mediapipe_classifier import MediaPipeAudioClassifier
from audio.capture import AudioFileLoader


def extract_enhanced_features(audio: np.ndarray, sr: int, classifier) -> np.ndarray:
    """
    YAMNet + ìŒí–¥ íŠ¹ì„± ì¶”ì¶œ

    Returns:
        117ì°¨ì› íŠ¹ì„± ë²¡í„°
    """
    # 1. YAMNet ì„ë² ë”© (88ì°¨ì›)
    yamnet_emb = classifier.extract_embedding(audio, sr)

    # 2. MFCC (Mel-frequency cepstral coefficients)
    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
    mfcc_mean = mfcc.mean(axis=1)  # 13ì°¨ì›
    mfcc_std = mfcc.std(axis=1)    # 13ì°¨ì›

    # 3. Spectral features
    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)
    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)
    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)

    # 4. Zero Crossing Rate (ê¸ˆì†ì„± ì†Œë¦¬ ê°ì§€ì— ìœ ìš©)
    zcr = librosa.feature.zero_crossing_rate(audio)

    # 5. RMS Energy
    rms = librosa.feature.rms(y=audio)

    # ê²°í•©
    features = np.concatenate([
        yamnet_emb,                      # 88ì°¨ì›
        mfcc_mean,                       # 13ì°¨ì›
        mfcc_std,                        # 13ì°¨ì›
        [spectral_centroid.mean()],      # 1ì°¨ì›
        [spectral_rolloff.mean()],       # 1ì°¨ì›
        [spectral_bandwidth.mean()],     # 1ì°¨ì›
        [zcr.mean()],                    # 1ì°¨ì›
        [rms.mean()]                     # 1ì°¨ì›
    ])  # ì´ 117ì°¨ì›

    return features


def main():
    print("="*80)
    print("ğŸ”Š One-Class v4 ëª¨ë¸ í•™ìŠµ (YAMNet + ìŒí–¥ íŠ¹ì„±)")
    print("="*80)

    # Paths
    knocking_dir = Path("data/training/engine_knocking_segments_v2")
    output_path = Path("data/models/car_classifier_oneclass_v4.pkl")
    yamnet_model = Path("data/models/yamnet.tflite")

    # Load YAMNet
    print("\nğŸ¤– YAMNet ë¡œë“œ ì¤‘...")
    classifier = MediaPipeAudioClassifier(
        model_path=str(yamnet_model),
        max_results=10,
        score_threshold=0.0
    )
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # Load data and extract enhanced features
    print(f"\nğŸ“‚ ë°ì´í„° ë¡œë”© ë° íŠ¹ì„± ì¶”ì¶œ: {knocking_dir}")
    print("="*80)

    audio_files = sorted(knocking_dir.glob("*.wav"))
    print(f"ğŸ”Š ì´ {len(audio_files)}ê°œ íŒŒì¼")
    print("   (YAMNet 88ì°¨ì› + ìŒí–¥ íŠ¹ì„± 29ì°¨ì› = 117ì°¨ì›)\n")

    X_list = []
    for i, audio_file in enumerate(audio_files, 1):
        try:
            # Load audio
            audio, sr = AudioFileLoader.load_audio(str(audio_file))

            # Extract enhanced features
            features = extract_enhanced_features(audio, sr, classifier)
            X_list.append(features)

            if i % 30 == 0 or i == len(audio_files):
                print(f"  âœ… [{i}/{len(audio_files)}] ì²˜ë¦¬ ì™„ë£Œ (íŠ¹ì„±: {len(features)}ì°¨ì›)")

        except Exception as e:
            print(f"  âš ï¸  {audio_file.name}: {e}")
            continue

    X = np.array(X_list)
    print(f"\nâœ… ì´ {len(X)}ê°œ ìƒ˜í”Œ ë¡œë“œ")
    print(f"   íŠ¹ì„± ì°¨ì›: {X.shape[1]}D (YAMNet 88 + ìŒí–¥ 29)")

    # Train
    print("\nğŸ¯ Isolation Forest í•™ìŠµ ì¤‘...")
    print("="*80)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    model = IsolationForest(
        n_estimators=100,
        contamination=0.1,
        max_samples='auto',
        random_state=42,
        n_jobs=-1
    )

    model.fit(X_scaled)
    print("âœ… í•™ìŠµ ì™„ë£Œ")

    # Evaluate
    predictions = model.predict(X_scaled)
    n_inliers = np.sum(predictions == 1)
    n_outliers = np.sum(predictions == -1)

    print(f"\nğŸ“Š í•™ìŠµ ë°ì´í„° í‰ê°€:")
    print(f"   - Inliers (ë…¸í‚¹ íŒ¨í„´): {n_inliers}ê°œ ({n_inliers/len(X)*100:.1f}%)")
    print(f"   - Outliers (ë…¸ì´ì¦ˆ): {n_outliers}ê°œ ({n_outliers/len(X)*100:.1f}%)")

    scores = model.decision_function(X_scaled)
    print(f"   - Decision score ë²”ìœ„: [{scores.min():.3f}, {scores.max():.3f}]")
    print(f"   - Decision score í‰ê· : {scores.mean():.3f}")

    # Save
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥: {output_path}")
    with open(output_path, 'wb') as f:
        pickle.dump({
            'model': model,
            'scaler': scaler,
            'model_type': 'one_class_enhanced',
            'info': {
                'n_samples': len(X),
                'n_features': X.shape[1],
                'features': 'YAMNet(88) + MFCC(26) + Spectral(3)',
                'model_type': 'isolation_forest',
                'description': 'One-Class with YAMNet + acoustic features',
            }
        }, f)

    print("âœ… ì €ì¥ ì™„ë£Œ!")

    print("\n"+"="*80)
    print("âœ… v4 ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("="*80)
    print(f"ğŸ“Š ëª¨ë¸ ì •ë³´:")
    print(f"   - ìƒ˜í”Œ: {len(X)}ê°œ")
    print(f"   - íŠ¹ì„±: {X.shape[1]}D (í–¥ìƒë¨!)")
    print(f"   - ì €ì¥: {output_path}")
    print("="*80)


if __name__ == "__main__":
    main()

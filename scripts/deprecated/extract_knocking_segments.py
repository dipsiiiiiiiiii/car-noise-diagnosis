#!/usr/bin/env python3
"""
Extract knocking segments from audio files using YAMNet classification

Usage:
    python extract_knocking_segments.py
"""

import sys
import numpy as np
from pathlib import Path
from typing import List, Tuple
import soundfile as sf

# Add src to path
sys.path.append(str(Path(__file__).parent / "src"))

from models.mediapipe_classifier import MediaPipeAudioClassifier
from audio.capture import AudioFileLoader


# YAMNet í‚¤ì›Œë“œ: ë…¸í‚¹ ê´€ë ¨ ì†Œë¦¬
KNOCKING_KEYWORDS = [
    'knock', 'knocking', 'tap', 'tapping',
    'rattle', 'rattling', 'clank', 'clanking',
    'mechanical', 'engine', 'motor',
    'clatter', 'bang', 'thump'
]


def calculate_knocking_score(classifications: List[dict]) -> float:
    """Calculate knocking relevance score from YAMNet results

    Args:
        classifications: YAMNet classification results

    Returns:
        Score between 0 and 1 (higher = more likely knocking)
    """
    max_score = 0.0

    for result in classifications:
        for category in result.get('categories', []):
            category_name = category['category_name'].lower()
            score = category['score']

            # Check if any knocking keyword matches
            for keyword in KNOCKING_KEYWORDS:
                if keyword in category_name:
                    max_score = max(max_score, score)
                    break

    return max_score


def extract_segments(audio_path: Path,
                     classifier: MediaPipeAudioClassifier,
                     output_dir: Path,
                     window_size: float = 3.0,
                     hop_size: float = 1.5,
                     threshold: float = 0.3) -> int:
    """Extract knocking segments from an audio file

    Args:
        audio_path: Path to input audio file
        classifier: YAMNet classifier
        output_dir: Directory to save extracted segments
        window_size: Window size in seconds (default: 3.0)
        hop_size: Hop size in seconds (default: 1.5)
        threshold: Minimum knocking score to save segment (default: 0.3)

    Returns:
        Number of segments extracted
    """
    print(f"\nğŸ“‚ Processing: {audio_path.name}")

    # Load audio
    try:
        audio_data, sample_rate = AudioFileLoader.load_audio(str(audio_path))
    except Exception as e:
        print(f"  âŒ Failed to load: {e}")
        return 0

    duration = len(audio_data) / sample_rate
    print(f"  â±ï¸  Duration: {duration:.1f}s, Sample Rate: {sample_rate}Hz")

    # Calculate window parameters
    window_samples = int(window_size * sample_rate)
    hop_samples = int(hop_size * sample_rate)

    segments_saved = 0
    base_name = audio_path.stem  # e.g., "knocking_01"

    # Sliding window
    for i, start_sample in enumerate(range(0, len(audio_data) - window_samples, hop_samples)):
        end_sample = start_sample + window_samples
        segment = audio_data[start_sample:end_sample]

        # Skip if too quiet
        rms = np.sqrt(np.mean(segment ** 2))
        if rms < 0.01:
            continue

        # Classify with YAMNet
        try:
            classifications = classifier.classify_audio(segment, sample_rate)
            knocking_score = calculate_knocking_score(classifications)

            # Get top prediction for logging
            top_pred = ""
            if classifications and classifications[0].get('categories'):
                top_cat = classifications[0]['categories'][0]
                top_pred = f"{top_cat['category_name']} ({top_cat['score']:.2f})"

            # Save if score exceeds threshold
            if knocking_score >= threshold:
                output_path = output_dir / f"{base_name}_seg_{segments_saved:03d}.wav"
                sf.write(str(output_path), segment, sample_rate)

                print(f"  âœ… Segment {segments_saved}: {start_sample/sample_rate:.1f}s-{end_sample/sample_rate:.1f}s "
                      f"(score: {knocking_score:.2f}, top: {top_pred})")
                segments_saved += 1
            else:
                # Optionally log skipped segments
                if i % 10 == 0:  # Log every 10th skip
                    print(f"  â­ï¸  Skip {start_sample/sample_rate:.1f}s (score: {knocking_score:.2f}, top: {top_pred})")

        except Exception as e:
            print(f"  âš ï¸  Classification error at {start_sample/sample_rate:.1f}s: {e}")
            continue

    print(f"  ğŸ’¾ Saved {segments_saved} segments")
    return segments_saved


def main():
    """Main extraction process"""
    print("=" * 80)
    print("ğŸ”Š YAMNet ê¸°ë°˜ ë…¸í‚¹ êµ¬ê°„ ìë™ ì¶”ì¶œ")
    print("=" * 80)

    # Paths
    input_dir = Path("data/training/engine_knocking")
    output_dir = Path("data/training/engine_knocking_segments")
    yamnet_model = Path("data/models/yamnet.tflite")

    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"\nğŸ“ Input:  {input_dir}")
    print(f"ğŸ“ Output: {output_dir}")

    # Check YAMNet model
    if not yamnet_model.exists():
        print(f"\nâŒ YAMNet ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {yamnet_model}")
        print("ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
        print("curl -L 'https://storage.googleapis.com/mediapipe-models/audio_classifier/yamnet/float32/1/yamnet.tflite' -o data/models/yamnet.tflite")
        return

    # Initialize YAMNet classifier
    print(f"\nğŸ¤– YAMNet ëª¨ë¸ ë¡œë“œ ì¤‘...")
    classifier = MediaPipeAudioClassifier(
        model_path=str(yamnet_model),
        max_results=10,
        score_threshold=0.0
    )
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # Find all WAV files
    audio_files = sorted(input_dir.glob("*.wav"))

    if not audio_files:
        print(f"\nâŒ {input_dir}ì—ì„œ WAV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return

    print(f"\nğŸ“Š ì´ {len(audio_files)}ê°œ íŒŒì¼ ë°œê²¬")

    # Process each file
    total_segments = 0
    for audio_file in audio_files:
        segments = extract_segments(
            audio_path=audio_file,
            classifier=classifier,
            output_dir=output_dir,
            window_size=3.0,      # 3ì´ˆ ìœˆë„ìš°
            hop_size=1.5,         # 1.5ì´ˆì”© ì´ë™ (50% overlap)
            threshold=0.3         # 30% ì´ìƒ ì‹ ë¢°ë„
        )
        total_segments += segments

    # Summary
    print("\n" + "=" * 80)
    print("âœ… ì¶”ì¶œ ì™„ë£Œ!")
    print("=" * 80)
    print(f"ğŸ“Š í†µê³„:")
    print(f"   - ì›ë³¸ íŒŒì¼: {len(audio_files)}ê°œ")
    print(f"   - ì¶”ì¶œëœ ë…¸í‚¹ êµ¬ê°„: {total_segments}ê°œ")
    print(f"   - ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. {output_dir}ì—ì„œ ì¶”ì¶œëœ íŒŒì¼ í™•ì¸")
    print(f"   2. ì˜ëª» ì¶”ì¶œëœ íŒŒì¼ì´ ìˆë‹¤ë©´ ìˆ˜ë™ìœ¼ë¡œ ì‚­ì œ")
    print(f"   3. python train.py ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ í•™ìŠµ")
    print("=" * 80)


if __name__ == "__main__":
    main()
